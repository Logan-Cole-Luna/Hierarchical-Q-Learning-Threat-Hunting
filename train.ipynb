{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:43:33.386043Z",
     "iopub.status.busy": "2023-11-28T16:43:33.385595Z",
     "iopub.status.idle": "2023-11-28T16:43:33.402173Z",
     "shell.execute_reply": "2023-11-28T16:43:33.400491Z",
     "shell.execute_reply.started": "2023-11-28T16:43:33.386007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Benign': 'Benign',\n",
       " 'Botnet': 'Botnet',\n",
       " 'Brute-force': 'Brute-force',\n",
       " 'DDoS attack': 'DDoS attack',\n",
       " 'DoS attack': 'DoS attack',\n",
       " 'Infilteration': 'Infilteration',\n",
       " 'Web attack': 'Web attack'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('label_dict.json') as json_file:\n",
    "    label_dict = json.load(json_file)\n",
    "\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:43:33.406853Z",
     "iopub.status.busy": "2023-11-28T16:43:33.406204Z",
     "iopub.status.idle": "2023-11-28T16:43:33.418057Z",
     "shell.execute_reply": "2023-11-28T16:43:33.41639Z",
     "shell.execute_reply.started": "2023-11-28T16:43:33.406812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0.2857142857142857,\n",
       " '1': 2.857142857142857,\n",
       " '2': 2.857142857142857,\n",
       " '3': 1.4285714285714286,\n",
       " '4': 1.4285714285714286,\n",
       " '5': 1.4285714285714286,\n",
       " '6': 1.4285714285714286}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('class_weights.json') as json_file:\n",
    "    class_weights = json.load(json_file)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:46.891135Z",
     "iopub.status.busy": "2023-11-28T16:55:46.890508Z",
     "iopub.status.idle": "2023-11-28T16:55:46.900595Z",
     "shell.execute_reply": "2023-11-28T16:55:46.899217Z",
     "shell.execute_reply.started": "2023-11-28T16:55:46.891092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K\n",
    "import logging.config\n",
    "from gym import  spaces\n",
    "import gym\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "use_subset = True  # Set to False to use the full dataset\n",
    "if use_subset:\n",
    "    train_df = pd.read_csv(\"processed_data/train_df_subset.csv\")\n",
    "    test_df = pd.read_csv(\"processed_data/train_df_subset.csv\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"processed_data/train_df.csv\")\n",
    "    test_df = pd.read_csv(\"processed_data/test_df.csv\")\n",
    "\n",
    "with open('processed_data/class_weights.json', 'r') as f:\n",
    "    class_weights = json.load(f)\n",
    "with open('processed_data/label_dict.json', 'r') as f:\n",
    "    label_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:47.48638Z",
     "iopub.status.busy": "2023-11-28T16:55:47.485652Z",
     "iopub.status.idle": "2023-11-28T16:55:47.504222Z",
     "shell.execute_reply": "2023-11-28T16:55:47.503139Z",
     "shell.execute_reply.started": "2023-11-28T16:55:47.486338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class data_cls:\n",
    "    def __init__(self, train_test, attack_map, **kwargs):\n",
    "        self.train_test = train_test\n",
    "        \n",
    "        if self.train_test == 'train':\n",
    "            self.train_path = \"train_df.csv\"\n",
    "        else:\n",
    "            self.test_path = \"test_df.csv\"\n",
    "\n",
    "        self.attack_map =   attack_map \n",
    "        self.attack_types = list(attack_map.keys())\n",
    "        \n",
    "        self.loaded = False\n",
    "    \n",
    "    def get_batch(self, batch_size=100):\n",
    "        if not self.loaded:\n",
    "            self._load_df()\n",
    "        \n",
    "        # Ensure batch_size does not exceed the DataFrame size\n",
    "        if batch_size > self.data_shape[0]:\n",
    "            raise ValueError(f\"batch_size ({batch_size}) cannot be larger than the dataset size ({self.data_shape[0]}).\")\n",
    "        \n",
    "        # Calculate wrapped indices using modulo\n",
    "        indexes = [(self.index + i) % self.data_shape[0] for i in range(batch_size)]\n",
    "        \n",
    "        # Update the index for the next batch\n",
    "        self.index = (self.index + batch_size) % self.data_shape[0]\n",
    "        \n",
    "        # Select the batch using iloc with valid indices\n",
    "        batch = self.df.iloc[indexes]\n",
    "        \n",
    "        map_type = pd.Series(index=self.attack_types, data=np.arange(len(self.attack_types))).to_dict()\n",
    "        labels = batch[label_col].map(self.attack_map).map(map_type).values\n",
    "        del batch[label_col]\n",
    "        \n",
    "        return np.array(batch), labels\n",
    "    \n",
    "    def get_full(self):\n",
    "\n",
    "        self._load_df()\n",
    "        \n",
    "        batch = self.df\n",
    "        map_type = pd.Series(index=self.attack_types,data=np.arange(len(self.attack_types))).to_dict()\n",
    "        labels = batch[label_col].map(self.attack_map).map(map_type).values\n",
    "        \n",
    "        del(batch[label_col])\n",
    "        \n",
    "        return np.array(batch), labels\n",
    "    \n",
    "    def get_shape(self):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "        \n",
    "        self.data_shape = self.df.shape\n",
    "        return self.data_shape\n",
    "    \n",
    "    def _load_df(self):\n",
    "        if self.train_test == 'train':\n",
    "            self.df = pd.read_csv(self.train_path) \n",
    "        else:\n",
    "            self.df = pd.read_csv(self.test_path)\n",
    "            \n",
    "        self.index=np.random.randint(0,self.df.shape[0]-1,dtype=np.int32)\n",
    "        self.loaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:47.506926Z",
     "iopub.status.busy": "2023-11-28T16:55:47.506249Z",
     "iopub.status.idle": "2023-11-28T16:55:47.522986Z",
     "shell.execute_reply": "2023-11-28T16:55:47.521501Z",
     "shell.execute_reply.started": "2023-11-28T16:55:47.506886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NetworkClassificationEnv(gym.Env, data_cls):\n",
    "    def __init__(self,train_test, attack_map, **kwargs):\n",
    "        data_cls.__init__(self,train_test, attack_map,**kwargs)\n",
    "        self.data_shape = self.get_shape()\n",
    "        self.batch_size = kwargs.get('batch_size', 1) \n",
    "        self.fails_episode = kwargs.get('fails_episode', 10) \n",
    "        \n",
    "        # Gym spaces\n",
    "        self.action_space = spaces.Discrete(len(self.attack_types))\n",
    "        self.observation_space = spaces.Discrete(self.data_shape[0])\n",
    "        \n",
    "        self.observation_len = self.data_shape[1]-1\n",
    "        \n",
    "        self.counter = 0\n",
    "\n",
    "    def _update_state(self):\n",
    "        self.states,self.labels = self.get_batch(self.batch_size)\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.states,self.labels = self.get_batch(self.batch_size)\n",
    "        self.counter = 0\n",
    "        \n",
    "        return self.states\n",
    "    \n",
    "    def _get_rewards(self,actions):\n",
    "        self.reward = 0\n",
    "        if actions == self.labels:\n",
    "            self.reward = 1\n",
    "        else: \n",
    "            self.counter += 1\n",
    "\n",
    "    def step(self,actions):\n",
    "        self._get_rewards(actions)\n",
    "            \n",
    "        self._update_state()\n",
    "\n",
    "        if self.counter >= self.fails_episode:\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.done = False\n",
    "            \n",
    "        return self.states, self.reward, self.done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:47.525902Z",
     "iopub.status.busy": "2023-11-28T16:55:47.5254Z",
     "iopub.status.idle": "2023-11-28T16:55:47.537347Z",
     "shell.execute_reply": "2023-11-28T16:55:47.536002Z",
     "shell.execute_reply.started": "2023-11-28T16:55:47.525863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def huber_loss(y_true, y_pred, clip_value=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < clip_value\n",
    "    squared_loss = 0.5 * tf.square(error)\n",
    "    linear_loss = clip_value * (tf.abs(error) - 0.5 * clip_value)\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "import keras.losses\n",
    "keras.losses.huber_loss = huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:48.002785Z",
     "iopub.status.busy": "2023-11-28T16:55:48.002081Z",
     "iopub.status.idle": "2023-11-28T16:55:48.045922Z",
     "shell.execute_reply": "2023-11-28T16:55:48.044615Z",
     "shell.execute_reply.started": "2023-11-28T16:55:48.002746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self,obs_size,num_actions, hidden_dense_layer_dict = {\"Dense_1\": {\"Size\": 100}}, learning_rate=0.001):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Input(shape=(obs_size,)))\n",
    "\n",
    "        for key, value in hidden_dense_layer_dict.items():\n",
    "            self.model.add(Dense(value[\"Size\"], activation='relu', name=key))\n",
    "\n",
    "        self.model.add(Dense(num_actions))\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate)\n",
    "        self.model.compile(loss=huber_loss,optimizer=optimizer)\n",
    "\n",
    "    def predict(self,state,batch_size=1):\n",
    "        return self.model.predict(state,batch_size=batch_size, verbose=0)\n",
    "\n",
    "    def update(self, states, q):\n",
    "        loss = self.model.train_on_batch(states, q, class_weight=class_weights)\n",
    "        return loss\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, num_actions, estimator):\n",
    "        self.num_actions = num_actions\n",
    "        self.estimator = estimator\n",
    "    \n",
    "class Epsilon_greedy(Policy):\n",
    "    def __init__(self,estimator ,num_actions,epsilon,decay_rate, epoch_length):\n",
    "        Policy.__init__(self, num_actions, estimator)\n",
    "        self.name = \"Epsilon Greedy\"\n",
    "        if (epsilon is None or epsilon < 0 or epsilon > 1):\n",
    "            print(\"EpsilonGreedy: Invalid value of epsilon\", flush = True)\n",
    "            sys.exit(0)\n",
    "        self.epsilon = epsilon\n",
    "        self.step_counter = 0\n",
    "        self.epoch_length = epoch_length\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon_decay = True\n",
    "        \n",
    "    def get_actions(self,states):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            actions = np.random.randint(0, self.num_actions,states.shape[0])\n",
    "        else:\n",
    "            self.Q = self.estimator.predict(states,states.shape[0])\n",
    "\n",
    "            actions = []\n",
    "            for row in range(self.Q.shape[0]):\n",
    "                best_actions = np.argwhere(self.Q[row] == np.amax(self.Q[row]))\n",
    "                actions.append(best_actions[np.random.choice(len(best_actions))].item())\n",
    "            \n",
    "        self.step_counter += 1 \n",
    "\n",
    "        if self.epsilon_decay:\n",
    "            if self.step_counter % self.epoch_length == 0:\n",
    "                self.epsilon = max(.01, self.epsilon * self.decay_rate**self.step_counter)\n",
    "            \n",
    "        return actions\n",
    "\n",
    "class Agent(object):   \n",
    "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
    "        \n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "        self.obs_size = obs_size\n",
    "        \n",
    "        self.epsilon = kwargs.get('epsilon', 1)\n",
    "        self.gamma = kwargs.get('gamma', 0.001)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 2)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        self.decay_rate = kwargs.get('decay_rate',0.99)\n",
    "        self.exp_rep = kwargs.get('exp_rep',True)\n",
    "        \n",
    "        if self.exp_rep:\n",
    "            self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10))\n",
    "        \n",
    "        self.ddqn_time = 100\n",
    "        self.ddqn_update = self.ddqn_time\n",
    "\n",
    "        self.model_network = QNetwork(self.obs_size, \n",
    "                                      self.num_actions,\n",
    "                                      kwargs.get('hidden_dense_layer_dict', {\"Dense_1\": {\"Size\": 100}}),\n",
    "                                      kwargs.get('learning_rate', 0.001))\n",
    "        \n",
    "        self.target_model_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                             kwargs.get('hidden_dense_layer_dict', {\"Dense_1\": {\"Size\": 100}}),\n",
    "                                             kwargs.get('learning_rate', 0.001))\n",
    "        \n",
    "        self.target_model_network.model.set_weights(self.model_network.model.get_weights()) \n",
    "        \n",
    "        if policy == \"EpsilonGreedy\":\n",
    "            self.policy = Epsilon_greedy(self.model_network,\n",
    "                                         len(actions),\n",
    "                                         self.epsilon,\n",
    "                                         self.decay_rate,\n",
    "                                         self.epoch_length)\n",
    "        \n",
    "    def act(self,states):\n",
    "        actions = self.policy.get_actions(states)\n",
    "        return actions\n",
    "    \n",
    "    def learn(self, states, actions,next_states, rewards, done):\n",
    "        if self.exp_rep:\n",
    "            self.memory.observe(states, actions, rewards, done)\n",
    "        else:\n",
    "            self.states = states\n",
    "            self.actions = actions\n",
    "            self.next_states = next_states\n",
    "            self.rewards = rewards\n",
    "            self.done = done\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        if self.exp_rep:\n",
    "            (states, actions, rewards, next_states, done) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "        else:\n",
    "            states = self.states\n",
    "            rewards = self.rewards\n",
    "            next_states = self.next_states\n",
    "            actions = self.actions\n",
    "            done = self.done\n",
    "            \n",
    "        next_actions = []\n",
    "        Q_prime = self.model_network.predict(next_states,self.minibatch_size)\n",
    "\n",
    "        for row in range(Q_prime.shape[0]):\n",
    "            best_next_actions = np.argwhere(Q_prime[row] == np.amax(Q_prime[row]))\n",
    "            next_actions.append(best_next_actions[np.random.choice(len(best_next_actions))].item())\n",
    "        sx = np.arange(len(next_actions))\n",
    "\n",
    "        Q = self.target_model_network.predict(states,self.minibatch_size)\n",
    "\n",
    "        targets = rewards.reshape(Q[sx,actions].shape) + \\\n",
    "                  self.gamma * Q_prime[sx,next_actions] * \\\n",
    "                  (1-done.reshape(Q[sx,actions].shape))   \n",
    "        Q[sx,actions] = targets  \n",
    "        \n",
    "        loss = self.model_network.model.train_on_batch(states,Q)\n",
    "        \n",
    "        self.ddqn_update -= 1\n",
    "        if self.ddqn_update == 0:\n",
    "            self.ddqn_update = self.ddqn_time\n",
    "            self.target_model_network.model.set_weights(self.model_network.model.get_weights()) \n",
    "        \n",
    "        return loss    \n",
    "    \n",
    "        \n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size, self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "\n",
    "        self.num_observed += 1\n",
    "\n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "\n",
    "        s      = np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32)\n",
    "        s_next = np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32)\n",
    "\n",
    "        a      = self.samples['action'][sampled_indices].reshape(minibatch_size)\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:48.048618Z",
     "iopub.status.busy": "2023-11-28T16:55:48.047959Z",
     "iopub.status.idle": "2023-11-28T16:55:48.061309Z",
     "shell.execute_reply": "2023-11-28T16:55:48.060019Z",
     "shell.execute_reply.started": "2023-11-28T16:55:48.048585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.isdir(\"models\"):\n",
    "    shutil.rmtree(\"models\", ignore_errors=False, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:48.09008Z",
     "iopub.status.busy": "2023-11-28T16:55:48.089644Z",
     "iopub.status.idle": "2023-11-28T16:55:48.096511Z",
     "shell.execute_reply": "2023-11-28T16:55:48.094856Z",
     "shell.execute_reply.started": "2023-11-28T16:55:48.090038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_col = 'Label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:55:48.444668Z",
     "iopub.status.busy": "2023-11-28T16:55:48.443115Z",
     "iopub.status.idle": "2023-11-28T16:56:18.351966Z",
     "shell.execute_reply": "2023-11-28T16:56:18.350368Z",
     "shell.execute_reply.started": "2023-11-28T16:55:48.444606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = \"models\"\n",
    "\n",
    "epsilon = 1  \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "minibatch_size = 100\n",
    "exp_rep = True\n",
    "\n",
    "iterations_episode = 100\n",
    "\n",
    "decay_rate = 0.99\n",
    "gamma = 0.001\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "hidden_dense_layer_dict = {\"Dense_2\": {\"Size\": 64},\n",
    "                           \"Dense_3\": {\"Size\": 32}\n",
    "                           }\n",
    "\n",
    "env = NetworkClassificationEnv('train',\n",
    "                                label_dict,\n",
    "                                batch_size = batch_size,\n",
    "                                iterations_episode = iterations_episode)\n",
    "\n",
    "# num_episodes = int(env.data_shape[0]/(iterations_episode)/10)\n",
    "num_episodes = 10\n",
    "valid_actions = list(range(len(env.attack_types)))\n",
    "num_actions = len(valid_actions)\n",
    "\n",
    "obs_size = env.observation_len\n",
    "\n",
    "agent = Agent(valid_actions,\n",
    "              obs_size,\n",
    "              \"EpsilonGreedy\",\n",
    "              learning_rate = learning_rate,\n",
    "              epoch_length = iterations_episode,\n",
    "              epsilon = epsilon,\n",
    "              decay_rate = decay_rate,\n",
    "              gamma = gamma,\n",
    "              hidden_dense_layer_dict = hidden_dense_layer_dict,\n",
    "              minibatch_size=minibatch_size,\n",
    "              mem_size = 10000,\n",
    "              exp_rep=exp_rep)    \n",
    "\n",
    "\n",
    "# Statistics\n",
    "reward_chain = []\n",
    "loss_chain = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T16:56:18.354754Z",
     "iopub.status.busy": "2023-11-28T16:56:18.354295Z",
     "iopub.status.idle": "2023-11-28T17:00:27.087916Z",
     "shell.execute_reply": "2023-11-28T17:00:27.085841Z",
     "shell.execute_reply.started": "2023-11-28T16:56:18.354717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Epoch 000/010 | Loss 0.0000 |Tot reward in ep 014| time: 0.04|\n",
      "|Estimated: [17. 16. 21. 10. 11. 13. 12.]|Labels: [58.  5.  3.  7.  8. 11.  8.]\n",
      "|Epoch 001/010 | Loss 0.7992 |Tot reward in ep 039| time: 13.81|\n",
      "|Estimated: [66.  5.  3.  8.  4.  6.  8.]|Labels: [54.  2.  6. 10.  9. 11.  8.]\n",
      "|Epoch 002/010 | Loss 0.8113 |Tot reward in ep 043| time: 13.99|\n",
      "|Estimated: [87.  4.  6.  0.  0.  3.  0.]|Labels: [46.  7.  8.  8.  9.  9. 13.]\n",
      "|Epoch 003/010 | Loss 0.7422 |Tot reward in ep 047| time: 16.56|\n",
      "|Estimated: [90.  4.  6.  0.  0.  0.  0.]|Labels: [46. 11.  4. 12.  8.  9. 10.]\n",
      "|Epoch 004/010 | Loss 0.6883 |Tot reward in ep 047| time: 14.68|\n",
      "|Estimated: [79.  7. 11.  1.  0.  0.  2.]|Labels: [49.  5.  4.  8. 17.  5. 12.]\n",
      "|Epoch 005/010 | Loss 0.6548 |Tot reward in ep 050| time: 14.45|\n",
      "|Estimated: [81.  3. 11.  0.  0.  0.  5.]|Labels: [46.  4.  3. 10. 12. 10. 15.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    loss = 0.\n",
    "    total_reward_by_episode = 0\n",
    "\n",
    "    states = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    true_labels = np.zeros(len(env.attack_types))\n",
    "    estimated_labels = np.zeros(len(env.attack_types))\n",
    "\n",
    "    for i_iteration in range(iterations_episode):\n",
    "        actions = agent.act(states)\n",
    "\n",
    "        estimated_labels[actions] += 1\n",
    "        true_labels[env.labels] += 1\n",
    "\n",
    "        next_states, reward, done = env.step(actions)\n",
    "        agent.learn(states, actions, next_states, reward, done)\n",
    "\n",
    "        if exp_rep and epoch*iterations_episode + i_iteration >= minibatch_size:\n",
    "            loss += agent.update_model()\n",
    "        elif not exp_rep:\n",
    "            loss += agent.update_model()\n",
    "\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        total_reward_by_episode += np.sum(reward, dtype=np.int32)\n",
    "\n",
    "    reward_chain.append(total_reward_by_episode)    \n",
    "    loss_chain.append(loss) \n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"\\r|Epoch {:03d}/{:03d} | Loss {:4.4f} |\" \n",
    "            \"Tot reward in ep {:03d}| time: {:2.2f}|\"\n",
    "            .format(epoch, num_episodes \n",
    "            ,loss, total_reward_by_episode,(end_time-start_time)))\n",
    "    print(\"\\r|Estimated: {}|Labels: {}\".format(estimated_labels,true_labels))\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "    \n",
    "agent.model_network.model.save_weights(\"models/DDQN_model.h5\", overwrite=True)\n",
    "with open(\"models/DDQN_model.json\", \"w\") as outfile:\n",
    "    json.dump(agent.model_network.model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T17:00:27.089934Z",
     "iopub.status.idle": "2023-11-28T17:00:27.090665Z",
     "shell.execute_reply": "2023-11-28T17:00:27.090339Z",
     "shell.execute_reply.started": "2023-11-28T17:00:27.090305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(len(reward_chain)),reward_chain)\n",
    "plt.title('Total reward by episode')\n",
    "plt.xlabel('n Episode')\n",
    "plt.ylabel('Total reward')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(len(loss_chain)),loss_chain)\n",
    "plt.title('Loss by episode')\n",
    "plt.xlabel('n Episode')\n",
    "plt.ylabel('loss')\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "    \n",
    "plt.savefig('results/train_type_improved.eps', format='eps', dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 902298,
     "sourceId": 1530359,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
